{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train data filtering: 100%|\u001b[32m██████████\u001b[0m| 88880/88880 [20:01<00:00, 73.96it/s]  \n",
      "validation data filtering: 100%|\u001b[32m██████████\u001b[0m| 9675/9675 [01:47<00:00, 89.78it/s]  \n",
      "test data filtering: 100%|\u001b[32m██████████\u001b[0m| 34680/34680 [01:06<00:00, 525.35it/s] \n",
      "train shard 1/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:20<00:00, 12.47it/s]\n",
      "train shard 2/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:27<00:00, 11.46it/s]\n",
      "train shard 3/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:53<00:00,  8.79it/s]\n",
      "train shard 4/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:19<00:00, 12.60it/s]\n",
      "train shard 5/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:40<00:00,  9.96it/s]\n",
      "train shard 6/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:26<00:00, 11.61it/s]\n",
      "train shard 7/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:30<00:00, 11.07it/s]\n",
      "train shard 8/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:38<00:00, 10.10it/s]\n",
      "train shard 9/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:37<00:00, 10.28it/s]\n",
      "train shard 10/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:46<00:00,  9.42it/s]\n",
      "train shard 11/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:32<00:00, 10.77it/s]\n",
      "train shard 12/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:39<00:00, 10.00it/s]\n",
      "train shard 13/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:48<00:00,  9.18it/s]\n",
      "train shard 14/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:19<00:00, 12.66it/s]\n",
      "train shard 15/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:51<00:00,  8.95it/s]\n",
      "train shard 16/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:36<00:00, 10.33it/s]\n",
      "train shard 17/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:43<00:00,  9.63it/s]\n",
      "train shard 18/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:52<00:00,  8.88it/s]\n",
      "train shard 19/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:24<00:00, 11.84it/s]\n",
      "train shard 20/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:29<00:00, 11.13it/s]\n",
      "train shard 21/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:36<00:00, 10.39it/s]\n",
      "train shard 22/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:27<00:00, 11.46it/s]\n",
      "train shard 23/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:28<00:00, 11.28it/s]\n",
      "train shard 24/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:31<00:00, 10.91it/s]\n",
      "train shard 25/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:21<00:00, 12.30it/s]\n",
      "train shard 26/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:43<00:00,  9.69it/s]\n",
      "train shard 27/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:29<00:00, 11.19it/s]\n",
      "train shard 28/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:34<00:00, 10.59it/s]\n",
      "train shard 29/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:26<00:00,  6.83it/s]\n",
      "train shard 30/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:10<00:00,  7.67it/s]\n",
      "train shard 31/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:33<00:00,  6.50it/s]\n",
      "train shard 32/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [03:14<00:00,  5.14it/s] \n",
      "train shard 33/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [05:41<00:00,  2.93it/s]\n",
      "train shard 34/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [04:07<00:00,  4.04it/s] \n",
      "train shard 35/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:48<00:00,  5.95it/s]\n",
      "train shard 36/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:08<00:00,  7.75it/s]\n",
      "train shard 37/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:06<00:00,  7.89it/s]\n",
      "train shard 38/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:55<00:00,  8.64it/s]\n",
      "train shard 39/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:03<00:00,  8.11it/s]\n",
      "train shard 40/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:32<00:00,  6.56it/s]\n",
      "train shard 41/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:38<00:00,  6.30it/s] \n",
      "train shard 42/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [03:10<00:00,  5.24it/s]\n",
      "train shard 43/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:35<00:00,  6.43it/s]\n",
      "train shard 44/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [03:46<00:00,  4.41it/s] \n",
      "train shard 45/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:28<00:00,  6.74it/s]\n",
      "train shard 46/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:45<00:00,  6.04it/s]\n",
      "train shard 47/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:42<00:00,  6.15it/s]\n",
      "train shard 48/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:44<00:00,  6.06it/s]\n",
      "train shard 49/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:47<00:00,  5.98it/s]\n",
      "train shard 50/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [03:04<00:00,  5.41it/s]\n",
      "train shard 51/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [03:00<00:00,  5.53it/s]\n",
      "train shard 52/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:58<00:00,  8.41it/s]\n",
      "train shard 53/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:27<00:00,  6.79it/s]\n",
      "train shard 54/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [03:32<00:00,  4.71it/s]\n",
      "train shard 55/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:24<00:00,  6.91it/s]\n",
      "train shard 56/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:16<00:00,  7.30it/s]\n",
      "train shard 57/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:31<00:00,  6.62it/s]\n",
      "train shard 58/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:45<00:00,  6.04it/s]\n",
      "train shard 59/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:00<00:00,  8.27it/s]\n",
      "train shard 60/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:09<00:00,  7.70it/s]\n",
      "train shard 61/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:58<00:00,  5.61it/s]\n",
      "train shard 62/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [03:32<00:00,  4.70it/s]\n",
      "train shard 63/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:32<00:00,  6.56it/s]\n",
      "train shard 64/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:56<00:00,  5.66it/s] \n",
      "train shard 65/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [03:42<00:00,  4.49it/s]\n",
      "train shard 66/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:17<00:00,  7.28it/s]\n",
      "train shard 67/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [03:27<00:00,  4.83it/s] \n",
      "train shard 68/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:40<00:00,  6.23it/s]\n",
      "train shard 69/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:54<00:00,  5.75it/s]\n",
      "train shard 70/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:40<00:00,  6.25it/s]\n",
      "train shard 71/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:20<00:00,  7.12it/s]\n",
      "train shard 72/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:31<00:00,  6.60it/s]\n",
      "train shard 73/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:57<00:00,  5.64it/s]\n",
      "train shard 74/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:43<00:00,  6.13it/s]\n",
      "train shard 75/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:41<00:00,  6.20it/s]\n",
      "train shard 76/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [03:07<00:00,  5.33it/s]\n",
      "train shard 77/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [03:22<00:00,  4.95it/s]\n",
      "train shard 78/79: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:48<00:00,  5.95it/s]\n",
      "train shard 79/79: 100%|\u001b[34m██████████\u001b[0m| 411/411 [00:52<00:00,  7.76it/s]\n",
      "validation shard 1/9: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:56<00:00,  5.67it/s]\n",
      "validation shard 2/9: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:12<00:00,  7.54it/s]\n",
      "validation shard 3/9: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [03:28<00:00,  4.79it/s]\n",
      "validation shard 4/9: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:57<00:00,  5.64it/s]\n",
      "validation shard 5/9: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [03:02<00:00,  5.48it/s]\n",
      "validation shard 6/9: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [03:05<00:00,  5.38it/s]\n",
      "validation shard 7/9: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:21<00:00,  7.06it/s]\n",
      "validation shard 8/9: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [03:05<00:00,  5.38it/s]\n",
      "validation shard 9/9: 100%|\u001b[34m██████████\u001b[0m| 935/935 [02:14<00:00,  6.95it/s]\n",
      "test shard 1/31: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:59<00:00,  8.37it/s]\n",
      "test shard 2/31: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:21<00:00, 12.25it/s]\n",
      "test shard 3/31: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [01:34<00:00, 10.55it/s]\n",
      "test shard 4/31: 100%|\u001b[34m██████████\u001b[0m| 1000/1000 [02:03<00:00,  8.10it/s]\n",
      "test shard 5/31:  74%|\u001b[34m███████▍  \u001b[0m| 744/1000 [01:06<00:23, 11.12it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 261\u001b[39m\n\u001b[32m    257\u001b[39m tensors_formation(data_split_path[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m], train_frame, \u001b[38;5;28mlist\u001b[39m(data_split_path.keys())[\u001b[32m0\u001b[39m])\n\u001b[32m    259\u001b[39m tensors_formation(data_split_path[\u001b[33m'\u001b[39m\u001b[33mvalidation\u001b[39m\u001b[33m'\u001b[39m], validation_frame, \u001b[38;5;28mlist\u001b[39m(data_split_path.keys())[\u001b[32m1\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m \u001b[43mtensors_formation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_split_path\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_split_path\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 223\u001b[39m, in \u001b[36mtensors_formation\u001b[39m\u001b[34m(data_base_path, data_frame, data_label, shard_size)\u001b[39m\n\u001b[32m    220\u001b[39m image_file_path = Path(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mgeneral_image_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    221\u001b[39m line_file_path = Path(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mgeneral_image_file_path.with_suffix(LINES_FILE_EXTENSIONS_FORMAT)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m mask, lanes = \u001b[43mdata_process_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m mask = resize_and_pad_mask(mask)\n\u001b[32m    225\u001b[39m serialized_example = serialize_example(\u001b[38;5;28mstr\u001b[39m(general_image_file_path), mask, lanes)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 105\u001b[39m, in \u001b[36mdata_process_mask\u001b[39m\u001b[34m(lines_file, image_file)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdata_process_mask\u001b[39m(lines_file, image_file):\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     mask_wdith, mask_height, _ = \u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_file\u001b[49m\u001b[43m)\u001b[49m.shape\n\u001b[32m    107\u001b[39m     lines = []\n\u001b[32m    109\u001b[39m     mask = np.zeros((mask_wdith, mask_height), dtype=np.uint8)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging as lg\n",
    "import datetime as dt\n",
    "import cv2 as cv\n",
    "import tensorflow as tf\n",
    "\n",
    "GLOBAL_TIMESTAMP = '{:%Y_%m_%d_%H_%M_%S}'.format(dt.datetime.now())\n",
    "\n",
    "LINES_FILE_EXTENSIONS_FORMAT = \".lines.txt\"\n",
    "\n",
    "BASE_PATH = \"../../\"\n",
    "\n",
    "OUTPUT_CSV_PATH = f\"{BASE_PATH}dataset-description\"\n",
    "\n",
    "data_split_path = {\"train\": f\"datasets/CuLane/train-validation\",\n",
    "                   \"validation\": f\"datasets/CuLane/train-validation\",\n",
    "                   \"test\": f\"datasets/CuLane/test\"}\n",
    "\n",
    "data_split_labels_path = {\"train\": f\"dataset-description/train.txt\",\n",
    "                          \"validation\": f\"dataset-description/val.txt\",\n",
    "                          \"test\": f\"dataset-description/test.txt\"}\n",
    "\n",
    "TRAIN_LOSS = VALIDATION_LOSS = TEST_LOSS = 0\n",
    "\n",
    "def data_frame_log(error_line, data_label):\n",
    "    global GLOBAL_TIMESTAMP\n",
    "\n",
    "    logging_path = f\"{OUTPUT_CSV_PATH}/data-filter-logging/{data_label}_{GLOBAL_TIMESTAMP}_.log\"\n",
    "\n",
    "    lg.basicConfig(filename=f\"{logging_path}\", filemode=\"w\", level=lg.INFO)\n",
    "\n",
    "    lg.info(f\"[{GLOBAL_TIMESTAMP}] | \" + error_line)\n",
    "\n",
    "def data_frames_obtain(train_frame, validation_frame, test_frame):\n",
    "    train_frame = pd.read_csv(f\"{BASE_PATH}{data_split_labels_path['train']}\", sep=\" \", header=None)\n",
    "\n",
    "    validation_frame = pd.read_csv(f\"{BASE_PATH}{data_split_labels_path['validation']}\", sep=\" \", header=None)\n",
    "\n",
    "    test_frame = pd.read_csv(f\"{BASE_PATH}{data_split_labels_path['test']}\", sep=\" \", header=None)\n",
    "\n",
    "    return train_frame, validation_frame, test_frame\n",
    "\n",
    "def data_frames_filter(data_base_path, data_frame, data_label):\n",
    "    global TRAIN_LOSS, VALIDATION_LOSS, TEST_LOSS, BASE_PATH\n",
    "\n",
    "    data_frame_copy = data_frame.copy()\n",
    "\n",
    "    Path(f\"{OUTPUT_CSV_PATH}/data-filter-logging\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for ind, path_frame in tqdm(enumerate(data_frame_copy[0]), total=len(data_frame_copy[0]), desc=f\"{data_label} data filtering\", colour=\"GREEN\"):\n",
    "        path = Path(f\"{BASE_PATH}{data_base_path}{path_frame}\")\n",
    "\n",
    "        path_lane = path.with_suffix(LINES_FILE_EXTENSIONS_FORMAT)\n",
    "\n",
    "        file_id = path.stem\n",
    "\n",
    "        id_record_flag = False\n",
    "\n",
    "        line_file_flag = False\n",
    "\n",
    "        if not path.exists():\n",
    "            data_frame_log(f\"Image file index: [{file_id}] | According to path [{path}] | Does not exist\", data_label)\n",
    "\n",
    "            id_record_flag = True\n",
    "        \n",
    "        elif not path_lane.exists():\n",
    "            data_frame_log(f\"Line file index: [{file_id}] | According to path [{path_lane}] | Does not exist\", data_label)\n",
    "\n",
    "            id_record_flag = True\n",
    "\n",
    "            line_file_flag = True\n",
    "        \n",
    "        elif path_lane.stat().st_size == 0:\n",
    "            data_frame_log(f\"Line file index: [{file_id}] | According to path [{path_lane}] | Is empty\", data_label)\n",
    "\n",
    "            id_record_flag = True\n",
    "\n",
    "            line_file_flag = True\n",
    "        \n",
    "        elif False == line_file_flag:\n",
    "            with open(path_lane, 'r') as file:\n",
    "                lines = [list(map(int, map(float, line.split()))) for line in file]\n",
    "            \n",
    "            if 2 > len(lines):\n",
    "                data_frame_log(f\"Line file index: [{file_id}] | According to path [{path_lane}] | Line quantity below minimum [2]\", data_label)\n",
    "\n",
    "                id_record_flag = True\n",
    "        \n",
    "        if id_record_flag:\n",
    "            data_frame.drop(ind, inplace=True)\n",
    "\n",
    "            if list(data_split_path.keys())[0] == data_label:\n",
    "                TRAIN_LOSS += 1\n",
    "            elif list(data_split_path.keys())[1] == data_label:\n",
    "                VALIDATION_LOSS += 1\n",
    "            else:\n",
    "                TEST_LOSS += 1\n",
    "\n",
    "    data_frame.reset_index(drop=True, inplace=True)\n",
    "\n",
    "def data_process_mask(lines_file, image_file):\n",
    "    mask_wdith, mask_height, _ = cv.imread(image_file).shape\n",
    "\n",
    "    lines = []\n",
    "\n",
    "    mask = np.zeros((mask_wdith, mask_height), dtype=np.uint8)\n",
    "\n",
    "    line_id = lane_id = coords_first_id = coords_second_id = 0\n",
    "\n",
    "    with open(lines_file, 'r') as file:\n",
    "        lines = [list(map(int, map(float, line.split()))) for line in file]\n",
    "    \n",
    "    while line_id != len(lines) - 1:\n",
    "        coords_first_id = coords_second_id = 0\n",
    "\n",
    "        while coords_first_id + 4 <= len(lines[line_id]) or coords_second_id + 4 <= len(lines[line_id + 1]):\n",
    "            x1, y1 = lines[line_id][coords_first_id], lines[line_id][coords_first_id + 1]\n",
    "            x2, y2 = lines[line_id][coords_first_id + 2], lines[line_id][coords_first_id + 3]\n",
    "            x3, y3 = lines[line_id + 1][coords_second_id], lines[line_id + 1][coords_second_id + 1]\n",
    "            x4, y4 = lines[line_id + 1][coords_second_id + 2], lines[line_id + 1][coords_second_id + 3]\n",
    "\n",
    "            cv.fillPoly(mask, [np.array([[x1, y1], [x2, y2], [x4, y4], [x3, y3]])], lane_id + 1, lineType = cv.LINE_AA)\n",
    "\n",
    "            if coords_first_id + 4 < len(lines[line_id]):\n",
    "                    coords_first_id += 2\n",
    "            \n",
    "            if coords_second_id + 4 < len(lines[line_id + 1]):\n",
    "                    coords_second_id += 2\n",
    "\n",
    "            if coords_first_id + 4 == len(lines[line_id]) and coords_second_id + 4 == len(lines[line_id + 1]):\n",
    "                    x1, y1 = lines[line_id][coords_first_id], lines[line_id][coords_first_id + 1]\n",
    "                    x2, y2 = lines[line_id][coords_first_id + 2], lines[line_id][coords_first_id + 3]\n",
    "                    x3, y3 = lines[line_id + 1][coords_second_id], lines[line_id + 1][coords_second_id + 1]\n",
    "                    x4, y4 = lines[line_id + 1][coords_second_id + 2], lines[line_id + 1][coords_second_id + 3]\n",
    "\n",
    "                    cv.fillPoly(mask, [np.array([[x1, y1], [x2, y2], [x4, y4], [x3, y3]])], lane_id + 1, lineType = cv.LINE_AA)\n",
    "\n",
    "                    break\n",
    "\n",
    "        line_id += 1\n",
    "\n",
    "        lane_id += 1\n",
    "\n",
    "    return np.stack([mask] * 3, axis=-1), lane_id\n",
    "\n",
    "def resize_and_pad_mask(mask, target_size=512, pad_value=255):\n",
    "    mask = mask[:, :, 0]\n",
    "\n",
    "    original_height, original_width = mask.shape\n",
    "\n",
    "    scale = min(target_size / original_height, target_size / original_width)\n",
    "    new_height = int(round(original_height * scale))\n",
    "    new_width = int(round(original_width * scale))\n",
    "\n",
    "    resized = cv.resize(mask, (new_width, new_height), interpolation=cv.INTER_NEAREST)\n",
    "\n",
    "    pad_top = (target_size - new_height) // 2\n",
    "    pad_bottom = target_size - new_height - pad_top\n",
    "    pad_left = (target_size - new_width) // 2\n",
    "    pad_right = target_size - new_width - pad_left\n",
    "\n",
    "    padded = cv.copyMakeBorder(\n",
    "        resized,\n",
    "        pad_top, pad_bottom,\n",
    "        pad_left, pad_right,\n",
    "        borderType=cv.BORDER_CONSTANT,\n",
    "        value=pad_value\n",
    "    )\n",
    "\n",
    "    return padded.astype(np.uint8)\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def serialize_example(image_path, mask_tensor, lane_quantity):\n",
    "    feature = {\n",
    "        'image_path': _bytes_feature(image_path.encode('utf-8')),\n",
    "        'mask_raw': _bytes_feature(mask_tensor.tobytes()),\n",
    "        'height': _int64_feature(mask_tensor.shape[0]),\n",
    "        'width': _int64_feature(mask_tensor.shape[1]),\n",
    "        # 'channels': _int64_feature(mask_tensor.shape[2]),\n",
    "        'lane_quantity': _int64_feature(lane_quantity)\n",
    "    }\n",
    "\n",
    "    # print(mask_tensor)\n",
    "\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "def tensors_formation(data_base_path, data_frame, data_label, shard_size=1000):\n",
    "    tfrecord_path = f\"{OUTPUT_CSV_PATH}/{data_label}.tfrecord\"\n",
    "    Path(tfrecord_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    tfrecord_data_label_path = f\"{OUTPUT_CSV_PATH}/{data_label}\"\n",
    "    Path(tfrecord_data_label_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    tfrecord_dir = Path(f\"{OUTPUT_CSV_PATH}/{data_label}\")\n",
    "    tfrecord_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    options = tf.io.TFRecordOptions(compression_type=\"ZLIB\")\n",
    "    total_samples = len(data_frame)\n",
    "    num_shards = (total_samples + shard_size - 1) // shard_size\n",
    "\n",
    "    for shard_id in range(num_shards):\n",
    "        start_idx = shard_id * shard_size\n",
    "        end_idx = min((shard_id + 1) * shard_size, total_samples)\n",
    "\n",
    "        shard_filename = tfrecord_dir / f\"{data_label}_{shard_id:05d}-of-{num_shards:05d}.tfrecord\"\n",
    "        with tf.io.TFRecordWriter(str(shard_filename), options=options) as writer:\n",
    "            for ind in tqdm(range(start_idx, end_idx), desc=f\"{data_label} shard {shard_id+1}/{num_shards}\", colour=\"BLUE\"):\n",
    "                try:\n",
    "                    general_image_file_path = Path(data_base_path + data_frame[0][ind])\n",
    "                    image_file_path = Path(f\"{BASE_PATH}{general_image_file_path}\")\n",
    "                    line_file_path = Path(f\"{BASE_PATH}{general_image_file_path.with_suffix(LINES_FILE_EXTENSIONS_FORMAT)}\")\n",
    "\n",
    "                    mask, lanes = data_process_mask(line_file_path, image_file_path)\n",
    "                    mask = resize_and_pad_mask(mask)\n",
    "                    serialized_example = serialize_example(str(general_image_file_path), mask, lanes)\n",
    "                    writer.write(serialized_example)\n",
    "                except Exception as e:\n",
    "                    data_frame_log(f\"Serialization error at index {ind}: {str(e)}\", data_label)\n",
    "\n",
    "    # with tf.io.TFRecordWriter(tfrecord_path, options=options) as writer:\n",
    "    #     for ind, elem in tqdm(enumerate(data_frame[0]), total=len(data_frame[0]), desc=f\"{data_label} TFRecord formation\", colour=\"BLUE\"):\n",
    "    #         general_image_file_path = Path(data_base_path + elem)\n",
    "    #         image_file_path = Path(f\"{BASE_PATH}{general_image_file_path}\")\n",
    "    #         line_file_path = Path(f\"{BASE_PATH}{general_image_file_path.with_suffix(LINES_FILE_EXTENSIONS_FORMAT)}\")\n",
    "\n",
    "    #         try:\n",
    "    #             mask, lanes = data_process_mask(line_file_path, image_file_path)\n",
    "    #             serialized_example = serialize_example(str(general_image_file_path), mask, lanes)\n",
    "    #             writer.write(serialized_example)\n",
    "    #         except Exception as e:\n",
    "    #             data_frame_log(f\"Serialization error at index {ind}: {str(e)}\", data_label)\n",
    "\n",
    "train_frame = validation_frame = test_frame = None\n",
    "\n",
    "train_frame, validation_frame, test_frame = data_frames_obtain(train_frame, validation_frame, test_frame)\n",
    "\n",
    "data_frames_filter(data_split_path['train'], train_frame, list(data_split_path.keys())[0])\n",
    "\n",
    "# data_frames_filter(data_split_path['validation'], validation_frame, list(data_split_path.keys())[1])\n",
    "\n",
    "# data_frames_filter(data_split_path['test'], test_frame, list(data_split_path.keys())[2])\n",
    "\n",
    "# statistics_df = pd.DataFrame({\"train_loss\": TRAIN_LOSS, \"validation_loss\": VALIDATION_LOSS, \"test_loss\": TEST_LOSS})\n",
    "\n",
    "# statistics_df.to_csv(f\"{OUTPUT_CSV_PATH}/loss_statistics.csv\")\n",
    "\n",
    "# tensors_formation(data_split_path['train'], train_frame, list(data_split_path.keys())[0])\n",
    "\n",
    "# tensors_formation(data_split_path['validation'], validation_frame, list(data_split_path.keys())[1])\n",
    "\n",
    "tensors_formation(data_split_path['test'], test_frame, list(data_split_path.keys())[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto-os",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
